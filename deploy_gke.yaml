resources:
  requests:                       # Minimum resources required.
    memory: "2Gi"
    cpu: "1"
  limits:                         # Maximum resources allowed
    memory: "12Gi"                # Maximum memory of the instance (80-90%)
    cpu: "4"                      # Maximum vCPUs of the instance


# Vertical scaling
---
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: llama-gke-deploy-vpa
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind:       Deployment
    name:       llama-gke-deploy
  updatePolicy:                   # Policy for updating the resource requests and limits
    updateMode: "Auto"            # Automatically update the resource requests and limits

# Horizontal scaling
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: llama-gke-deploy-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: llama-gke-deploy
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource                # Type of metric
    resource:                     # Resource-based metric
      name: cpu                   # Metric name
      target:
        type: Utilization         # Type of target value
        averageUtilization: 70    # Average CPU utilization percentage to maintain

readinessProbe:             # Check if the pod is ready to serve traffic.
  httpGet:
    scheme: HTTP
    path: /
    port: 8000              # Port for readiness probe (should match containerPort)
  initialDelaySeconds: 240  # Delay before first probe is executed
  periodSeconds: 60         # Interval between probes

livenessProbe:              # Check if the pod is alive
  httpGet:
    scheme: HTTP
    path: /
    port: 8000              # Port for liveness probe (should match containerPort)
  initialDelaySeconds: 240  # Delay before first probe is executed
  periodSeconds: 60         # Interval between probes